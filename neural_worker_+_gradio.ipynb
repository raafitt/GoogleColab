{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raafitt/GoogleColab/blob/neural-worker/neural_worker_%2B_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Нейросотрудники представляют собой рекрутеров Озон и Т-Банк. Обучаются на табличных данных"
      ],
      "metadata": {
        "id": "p_GON0kNF2nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai gradio tiktoken langchain langchain-openai langchain-community chromadb faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sLQVCPdv8TT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "              {\n",
        "                \"doc\": \"https://docs.google.com/spreadsheets/d/1_R3hZN6M5STYqK2sCh9vtlC0m7L5s8g_/edit?gid=1950053208#gid=1950053208\",\n",
        "                \"prompt\": '''Ты менеджер по подбору персонала Т-Банк, к тебе могут обращаться соискатели с вопросами по поводу вакансий. Перед тобой документ, в котором есть колонки \"Название вакансии\",\"График работы\",\"Формат работы\",\n",
        "                \"Тип оформления\",\"Бенефиты для сотрудников\",\"Чем предстоит заниматься\",\"Кого мы ищем\"\n",
        "                        Отвечай на вопросы на основе данных в документе, от себя ничего не выдумывай: ''',\n",
        "                \"name\": \"рекрутер Т-Банк\",\n",
        "                \"query\": \"Какие вакансии есть?\"\n",
        "              },\n",
        "\n",
        "               {\n",
        "                \"doc\": \"https://docs.google.com/spreadsheets/d/1UZHbBCdegN694atmKDgFEYFzos89JlXuylFfqHy0tJo/edit?usp=sharing\",\n",
        "                \"prompt\": '''Ты сотрудник по подбору персонала. Перед тобой документ, в котором есть колонки \"Адрес\", \"Средний пробег на курьера в день\",\"Ср.доход в день\",\"Время оказания услуги\",\n",
        "                \"Подработка\",\"Смена\",\"Вакансия Авто, количество\", \"Вакансия велосипед, количество\" в которых содержится информация для курьера Вкус Вилл. Документ в формате csv, структурируй его.\n",
        "                Если запрошенного пользователя нет в документе, кратко отвечай, что в данный момент информация по данному городу отсутствует\n",
        "                        Твоя задача ответить на вопросы сосискателя используя информацию из документа.\n",
        "                        Документ: ''',\n",
        "                \"name\": \"рекрутер Озон\",\n",
        "                \"query\": \"Какой график работы в Москве?\"\n",
        "              },\n",
        "\n",
        "            ]\n"
      ],
      "metadata": {
        "id": "TFrc6TZ13FmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass # для работы с паролями\n",
        "import os      # для работы с окружением и файловой системой\n",
        "\n",
        "# Запрос ввода ключа от OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1mOp-J09m53",
        "outputId": "42591cb8-a6b5-4665-d0fa-6de9f5b0316c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Блок библиотек фреймворка LangChain\n",
        "\n",
        "# Работа с документами в langchain\n",
        "from langchain.docstore.document import Document\n",
        "# Эмбеддинги для OpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# Доступ к векторной базе данных\n",
        "from langchain.vectorstores import Chroma\n",
        "# Разделение текста на куски или чанки (chunk)\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import chromadb\n",
        "# Отправка запросов\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Доступ к OpenAI\n",
        "from openai import OpenAI\n",
        "\n",
        "# Отприсовка интерфейса с помощью grad\n",
        "import gradio as gr\n",
        "\n",
        "# Библиотека подсчёта токенов\n",
        "# Без запроcов к OpenAI, тем самым не тратим деньги на запросы\n",
        "import tiktoken\n",
        "import faiss\n",
        "# Для работы с регулярными выражениями\n",
        "import re"
      ],
      "metadata": {
        "id": "zxL6n01r94yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для оптимизации улучшен поиск в векторном пространстве, для этого применен фреймворк FAISS. Facebook AI Research Similarity Search – разработка команды Facebook AI Research для быстрого поиска ближайших соседей и кластеризации в векторном пространстве. Метод create_embedding теперь создает и сохраняет вектора и добавляет их в FAISS.\n",
        "\n",
        "В связи с особенностью предоставления данных таблицами, подаваемые на вход модели фрагменты одноуровневые, так как в таблицах информация может повторяться"
      ],
      "metadata": {
        "id": "P3a1kVAMC_9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Объявляем класс нейро-сотрудника\n",
        "class GPT():\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\"):\n",
        "        self.log = ''\n",
        "        self.model = model\n",
        "        self.search_index = None\n",
        "        self.embedded_docs = []  # Хранение документов для последующего извлечения по индексу\n",
        "        self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    def load_search_indexes(self, url):\n",
        "        # Преобразование URL Google Sheets в CSV\n",
        "        pattern = r'https://docs\\.google\\.com/spreadsheets/d/([a-zA-Z0-9-_]+)(/edit#gid=(\\d+)|/edit.*)?'\n",
        "        replacement = lambda m: f'https://docs.google.com/spreadsheets/d/{m.group(1)}/export?' + (f'gid={m.group(3)}&' if m.group(3) else '') + 'format=csv'\n",
        "        new_url = re.sub(pattern, replacement, url)\n",
        "\n",
        "        # Чтение данных и преобразование в строки\n",
        "        rows_as_strings = ''\n",
        "        df = pd.read_csv(new_url)\n",
        "        for _, row in df.iterrows():\n",
        "            row_str = \", \".join([f\"{col}: {row[col]}\" for col in df.columns])\n",
        "            rows_as_strings += 'следующая вакансия ' + row_str\n",
        "\n",
        "        # Создание векторного индекса с использованием FAISS\n",
        "        return self.create_embedding(rows_as_strings)\n",
        "\n",
        "\n",
        "    def num_tokens_from_string(self, string):\n",
        "        encoding = encoding_for_model(self.model)\n",
        "        return len(encoding.encode(string))\n",
        "\n",
        "    def create_embedding(self, data):\n",
        "        # Разбивка данных на части\n",
        "        source_chunks = []\n",
        "        splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=2048, chunk_overlap=0)\n",
        "        for chunk in splitter.split_text(data):\n",
        "            doc = Document(page_content=chunk, metadata={})\n",
        "            source_chunks.append(doc)\n",
        "            self.embedded_docs.append(doc)  # Добавляем документ в список для последующего доступа\n",
        "\n",
        "        # Создание и добавление векторов в FAISS\n",
        "        embeddings = [OpenAIEmbeddings().embed_query(doc.page_content) for doc in source_chunks]\n",
        "        dimension = len(embeddings[0])\n",
        "        self.search_index = faiss.IndexFlatL2(dimension) #Задание размерности входных векторов\n",
        "        self.search_index.add(np.array(embeddings).astype('float32')) #добавление векторов в базу\n",
        "\n",
        "        self.log += f'Количество токенов в документе: {self.num_tokens_from_string(\" \".join([x.page_content for x in source_chunks]))}\\n'\n",
        "        self.log += 'Данные из документа загружены в векторную базу данных и FAISS индекс\\n'\n",
        "        return self.search_index\n",
        "\n",
        "    def answer_index(self, system, topic, temp=1):\n",
        "        if not self.search_index or not self.search_index:\n",
        "            self.log += 'Модель необходимо обучить!\\n'\n",
        "            return ''\n",
        "\n",
        "        # Поиск по векторной базе FAISS\n",
        "        query_embedding = OpenAIEmbeddings().embed_query(topic)\n",
        "        D, I = self.search_index.search(np.array([query_embedding]).astype('float32'), k=5) #Возвращает результат: дистанции и индексы\n",
        "\n",
        "        # Получение документов по идентификаторам из базы\n",
        "        docs = [self.embedded_docs[i] for i in I[0]]\n",
        "        message_content = f\"Описание вакансий. Найденная информация из базы: {[doc.page_content for doc in docs]}\"\n",
        "\n",
        "        self.log += f'message_content={message_content}\\n'\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"{system}\\n{message_content}\"},\n",
        "            {\"role\": \"user\", \"content\": topic}\n",
        "        ]\n",
        "\n",
        "        # Запрос к языковой модели\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            temperature=temp\n",
        "        )\n",
        "\n",
        "        self.log += f'\\nТокенов использовано всего (вопрос): {completion.usage.prompt_tokens}\\n'\n",
        "        self.log += f'Токенов использовано всего (вопрос-ответ): {completion.usage.total_tokens}\\n'\n",
        "\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "SayH4PeUHRT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gradio as gr\n",
        "\n",
        "# Объявляем экземпляр класса GPT и передаем ему в конструктор модель LLM\n",
        "gpt = GPT(\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "# Gradio позволяет объединять элементы в блоки\n",
        "blocks = gr.Blocks()\n",
        "\n",
        "# Работаем с блоком\n",
        "with blocks as demo:\n",
        "    # Объявляем элемент выбор из списка\n",
        "    subject = gr.Dropdown([(elem[\"name\"], index) for index, elem in enumerate(models)], label=\"Данные\")\n",
        "    # Поля для отображения информации, связанной с выбранной моделью\n",
        "    name = gr.Label(show_label=False)\n",
        "    prompt = gr.Textbox(label=\"Промт\", interactive=True)\n",
        "    link = gr.HTML()\n",
        "    query = gr.Textbox(label=\"Запрос к LLM\", interactive=True)\n",
        "\n",
        "    # Функция на выбор нейро-сотрудника в models\n",
        "    def onchange(dropdown):\n",
        "        try:\n",
        "            # Проверка, что индекс выбора корректен\n",
        "            if dropdown not in range(len(models)):\n",
        "                raise ValueError(\"Некорректное значение выбора. Пожалуйста, выберите модель из списка.\")\n",
        "\n",
        "            # Получение данных из models\n",
        "            selected_model = models[dropdown]\n",
        "            return [\n",
        "                selected_model['name'],\n",
        "                re.sub(r'\\t+|\\s\\s+', ' ', selected_model['prompt']),\n",
        "                selected_model['query'],\n",
        "                f\"<a target='_blank' href='{selected_model['doc']}'>Документ для обучения</a>\"\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при выборе модели: {e}\")\n",
        "            return [\"Ошибка\", \"Ошибка\", \"Ошибка\", \"\"]\n",
        "\n",
        "    # При изменении значения в поле списка subject вызывается функция onchange\n",
        "    subject.change(onchange, inputs=[subject], outputs=[name, prompt, query, link])\n",
        "\n",
        "    # Строку в gradio можно разделить на столбцы (каждая кнопка в своем столбце)\n",
        "    with gr.Row():\n",
        "        train_btn = gr.Button(\"Обучить модель\")\n",
        "        request_btn = gr.Button(\"Запрос к модели\")\n",
        "\n",
        "    # Функция обучения\n",
        "    def train(dropdown):\n",
        "        try:\n",
        "            # Проверка корректности индекса выбора модели\n",
        "            if dropdown not in range(len(models)):\n",
        "                raise ValueError(\"Некорректное значение выбора. Пожалуйста, выберите модель из списка.\")\n",
        "\n",
        "            # Загрузка документа и логгирование\n",
        "            gpt.load_search_indexes(models[dropdown]['doc'])\n",
        "            return gpt.log\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обучении модели: {e}\")\n",
        "            return \"Ошибка при обучении. Проверьте выбранные данные и повторите попытку.\"\n",
        "\n",
        "    # Вызываем метод запроса к языковой модели из класса GPT\n",
        "    def predict(p, q):\n",
        "      try:\n",
        "          # Проверка корректности полей prompt и query\n",
        "          if not p.strip():\n",
        "              raise ValueError(\"Поле промта не может быть пустым.\")\n",
        "          if not q.strip():\n",
        "              raise ValueError(\"Поле запроса не может быть пустым.\")\n",
        "\n",
        "          # Вызов метода ответа от LLM\n",
        "          result = gpt.answer_index(p, q)\n",
        "\n",
        "          # Проверка успешности ответа\n",
        "          if not result:\n",
        "              raise RuntimeError(\"Ответ от модели пустой. Проверьте запрос и попробуйте снова.\")\n",
        "\n",
        "          # возвращает список из ответа от LLM и log от класса GPT\n",
        "          return [result, gpt.log]\n",
        "\n",
        "      except ValueError as ve:\n",
        "          error_msg = f\"Ошибка ввода: {ve}\"\n",
        "          print(error_msg)\n",
        "          return [error_msg, \"\"]\n",
        "\n",
        "      except RuntimeError as re:\n",
        "          error_msg = f\"Ошибка при получении ответа от LLM: {re}\"\n",
        "          print(error_msg)\n",
        "          return [error_msg, gpt.log]\n",
        "\n",
        "      except Exception as e:\n",
        "          error_msg = f\"Неизвестная ошибка при запросе к модели: {e}\"\n",
        "          print(error_msg)\n",
        "          return [error_msg, gpt.log]\n",
        "\n",
        "    # Выводим поля response с ответом от LLM и log (вывод сообщений работы класса GPT) на 2 колонки\n",
        "    with gr.Row():\n",
        "        response = gr.Textbox(label=\"Ответ LLM\") # Текстовое поле с ответом от LLM\n",
        "        log = gr.Textbox(label=\"Логирование\")    # Текстовое поле с выводом сообщений от GPT\n",
        "\n",
        "\n",
        "    # При нажатии на кнопку train_btn запускается функция обучения train_btn с параметром subject\n",
        "    # Результат выполнения функции сохраняем в текстовое поле log - лог выполнения\n",
        "    train_btn.click(train, [subject], log)\n",
        "\n",
        "    # При нажатии на кнопку request_btn запускается функция отправки запроса к LLM request_btn с параметром prompt, query\n",
        "    # Результат выполнения функции сохраняем в текстовые поля  response - ответ модели, log - лог выполнения\n",
        "    request_btn.click(predict, [prompt, query], [response, log])\n",
        "\n",
        "# Запуск приложения\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "q-3agBBfH5KE",
        "outputId": "584f0b60-a61e-4089-d6cc-95cfcfaa0e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f80aed1e9cfe6f7a1a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f80aed1e9cfe6f7a1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}